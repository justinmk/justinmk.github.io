---
date: "2010-10-05T00:00:00Z"
published: true
title: Learning Asymptote
---

<i>Learning without thought is labor lost; thought without learning is perilous.</i>

<i>- Confucius</i>

It is said that some people are "quick learners". Does that mean they can learn anything quickly? No step in the path of learning challenges them? No; rather, they learn quickly compared to others, but that quickness is nevertheless measured in mortal units.

For slow and quick learners alike, the process of learning requires <i>some</i> time and concentration. Over time, you can master your shortcomings and adapt to a less-awkward process in order to reduce friction; yet learning continues to be even for the shrewdest autodidact frustrating and slow, compared to what it might be if we had a less-cobwebbed conduit by which to hose the <a href="http://en.wikipedia.org/wiki/Hippocampus">inner sanctum</a> of cognitive retention.

Learning might be improved, perhaps by orders of magnitude, when genetic engineering creates a super-human. Yet even should that scenario play out as supposed, learning capacity will necessarily reach new limits which, hitherto unimagined, thenceforth become the status quo; and though the supernatural humans of that imagined future regard us with pity, even for them learning quickly settles into its habit of frustration and finitude.

That is, learning is frustrating because it is a challenge of limits. To operate comfortably within limits is to achieve very nearly an <i>idle state</i>; in contrast, to challenge a limit is to compound resistance exponentially. The concepts I am trying to crowbar into my brain right now might seem, ultimately, simple—but that is incidental. They seem hard <i>right now</i> because I'm re-tooling a neural net; once that neural net is built, the limit is relocated, and the concepts become familiar.

This is why <a href="http://en.wikipedia.org/wiki/Computational_complexity_theory">worst-case scenarios</a> matter. Two-billion megahertz are more than you need, <i>except when you need more</i>. Complexity analysis might appear to be rigmarole, but it is rigor—from which we may have nice things like <a href="http://en.wikipedia.org/wiki/Hash_function">the index</a> and <a href="http://www.google.com/">the search engine</a>.

In the language of complexity theory, human brain power is a <i>constant.</i> No matter how much you pump up that constant via genetic engineering or alien surgery, the algorithm—intelligence—has bounds. So, learning is unwieldy—but that is a _subjective judgement_ that scales with your brain resources.

The question is: Do you want to idle, or do you want to peg your processor?
